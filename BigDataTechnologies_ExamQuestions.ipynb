{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNn+BxPK5o3PRq8zbxvTVuu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSimple07/Big_Data_Technologies/blob/main/BigDataTechnologies_ExamQuestions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Consepts of Big Data"
      ],
      "metadata": {
        "id": "QLxze36700nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.What is Big Data? History of Big Data, V-concepts\n",
        "\n",
        "Big Data is the large data information that grow at ever- increasing rates.\n",
        "\n",
        "Big data is characterized by 3 V's - Volume, Velocity, Variety\n",
        "\n",
        "- Volume - The quantity of generated and stored data.\n",
        "- Velocity - The speed at which new data is generated and moved\n",
        "- Variety  - The different types of data, both structured and unstructured.\n",
        "\n",
        "Over time 2 V's also added:\n",
        "- Veracity - The quality of data\n",
        "- Value - The usefullness of data\n",
        "\n",
        "And also:\n",
        "- Visualization\n",
        "- Vulnerability: The security of data\n",
        "- Volatility - The lifespan of data. How long data is valid and useful\n",
        "- Variability - The inconsistency of data flows.\n",
        "\n",
        "1990th years \"Big Data\" term gains popularity. The first statistical data analysis used in 1663 year by John Graunt.\n",
        "\n",
        "2000s - Google published its paper on the MapReduce algorithm. And Apache Hadoop introduced and it was revolutionary."
      ],
      "metadata": {
        "id": "y8CQLtF9WW4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.Streaming solutions for Big Data\n",
        "Streaming data means a continuous flow of data from various sources – data streams can be processed, stored and analyzed using special technologies as it is generated in real-time. The goal of this streaming is to ensure a constant flow of data to be processed without needing to download it from the source first.\n",
        "\n",
        "\n",
        "Streaming solution means that analyzing data in the real time. They are applied for different applications:\n",
        "\n",
        "- Fraud Detection: Real - Time monitoring of transactions to detect fraud actions\n",
        "- IoT Data Processing\n",
        "- Recommendation Engines: Offering real-time personalized recommendations.\n",
        "\n",
        "Streaming Solutions for Big Data:\n",
        "\n",
        "   1. **Apache Kafka**\n",
        "\n",
        "      Apache Kafka is a distributed streaming platform that is used for building real-time data pipelines and streaming applications. It is highly scalable, fault-tolerant, and provides low-latency messaging.\n",
        "\n",
        "   2. **Apache Flink**\n",
        "\n",
        "      Apache Flink is a distributed stream processing framework with powerful event-time processing capabilities. It supports both batch and stream processing, making it suitable for real-time analytics and data-driven applications.\n",
        "\n",
        "   3. **Apache Spark Streaming**\n",
        "\n",
        "      Apache Spark Streaming is an extension of the Apache Spark platform that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. It provides a simple API for transforming and manipulating streaming data.\n",
        "\n",
        "   4. **Amazon Kinesis**\n",
        "\n",
        "      Amazon Kinesis is a managed service for real-time processing of streaming data at scale. It offers capabilities for data ingestion, processing, and analysis with low latency and high throughput.\n",
        "\n",
        "   5. **Google Cloud Dataflow**\n",
        "\n",
        "      Google Cloud Dataflow is a fully managed service for real-time stream processing and batch processing of data. It provides unified programming model for both batch and stream data processing jobs.\n",
        "\n",
        "   6. **Apache Storm**\n",
        "\n",
        "      Apache Storm is a distributed real-time computation system for processing fast, large streams of data. It is scalable, fault-tolerant, and supports various programming languages for defining data processing logic.\n",
        "\n",
        "   7. **Confluent Platform**\n",
        "\n",
        "      Confluent Platform is built on top of Apache Kafka and provides additional features and tools for building and managing streaming data pipelines. It includes schema registry, connectors, and stream processing capabilities.\n",
        "\n",
        "   8. **Azure Stream Analytics**\n",
        "\n",
        "      Azure Stream Analytics is a fully managed real-time analytics service for processing streaming data from devices, sensors, web sites, social media, and other sources. It integrates with Azure services for seamless data integration and analysis.\n",
        "-->"
      ],
      "metadata": {
        "id": "OIyiLWo5XzWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.MapReduce concept and its main steps.\n",
        "\n",
        "Distributed computing model for processing Big Data. We can implement an algorithm in map reduce paradigm, used wen classic algorithm struggles by memory\n",
        "\n",
        "1. Map - data processing.\n",
        "2. Reduce - data convolution.\n",
        "\n",
        "Input files, are splitted so mappers (workers on stage Map) stores data on drive. Going to Reduce stage, data procced by mappers is stored as a key->value. Data with equal keys comes on the same reducer (worker on stage Reduce) Result stored in output files\n",
        "\n",
        "Input data should be splittable (split = block in hdfs hadoop distributed), data in split should be independent. One worker process one split. Middleware data stores on local disk. For each Reducer, Mapper creates Key->Value. Data with the same Key goes into one Reducer. Reducers start working when all mappers finished processing. Data from Reducer \"N\" is written output file \"N\", which are stored in hdfs."
      ],
      "metadata": {
        "id": "ZwgFc_nLYE5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storages and resource managers - supplementary tools\n",
        "\n",
        "   Storage Solutions:\n",
        "\n",
        "   1. **Hadoop Distributed File System (HDFS)**\n",
        "\n",
        "      HDFS is a distributed file system designed to store large datasets across commodity hardware. It provides high-throughput access to application data and is a core component of the Apache Hadoop project.\n",
        "\n",
        "   2. **Amazon S3 (Simple Storage Service)**\n",
        "\n",
        "      Amazon S3 is an object storage service that offers scalability, data availability, security, and performance. It is widely used for storing and retrieving any amount of data from anywhere on the web.\n",
        "\n",
        "   3. **Google Cloud Storage**\n",
        "\n",
        "      Google Cloud Storage is a scalable object storage service designed to store large unstructured data. It offers durability, availability, and flexibility, making it suitable for a wide range of applications.\n",
        "\n",
        "   4. **Azure Blob Storage**\n",
        "\n",
        "      Azure Blob Storage is Microsoft's object storage solution for the cloud. It provides scalable storage for unstructured data and offers features like data encryption, access tiers, and lifecycle management.\n",
        "\n",
        "   5. **Apache Cassandra**\n",
        "\n",
        "      Apache Cassandra is a distributed NoSQL database designed for handling large amounts of data across multiple servers. It provides high availability, fault tolerance, and linear scalability.\n",
        "\n",
        "   6. **Apache HBase**\n",
        "\n",
        "      Apache HBase is a distributed, scalable, big data store modeled after Google Bigtable. It provides random, real-time read/write access to large datasets and integrates well with Apache Hadoop.\n",
        "\n",
        "   7. **Elasticsearch**\n",
        "\n",
        "      Elasticsearch is a distributed, RESTful search and analytics engine designed for horizontal scalability, reliability, and real-time search. It is commonly used for log analytics, full-text search, and other use cases.\n",
        "\n",
        "\n",
        " Resource Managers:\n",
        "\n",
        "   1. **Apache YARN (Yet another Resource Negotiator)**\n",
        "\n",
        "      Apache YARN is a resource management layer in Hadoop that manages resources and schedules tasks across a Hadoop cluster. It separates resource management from processing engines, enabling multiple data processing frameworks to run on the same data.\n",
        "\n",
        "   2. **Apache Mesos**\n",
        "\n",
        "      Apache Mesos is a distributed systems kernel that abstracts CPU, memory, storage, and other resources to support the deployment and scaling of applications. It provides efficient resource isolation and sharing across distributed applications.\n",
        "\n",
        "   3. **Kubernetes**\n",
        "\n",
        "      Kubernetes is an open-source container orchestration platform for automating deployment, scaling, and management of containerized applications. It provides features like service discovery, load balancing, and automated rollouts and rollbacks.\n",
        "\n",
        "   4. **Apache ZooKeeper**\n",
        "\n",
        "      Apache ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and group services. It is used for coordinating distributed applications and managing distributed resources."
      ],
      "metadata": {
        "id": "RREnwOjrbEmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is HDFS? HDFS Architecture. What is a chunk? How are chunks being processed into records? How to customize the way chunks are being processed?\n",
        "\n",
        "\n",
        "### Hadoop Distributed File System (HDFS)\n",
        "\n",
        "**HDFS** (Hadoop Distributed File System) is a distributed file system designed to store large datasets reliably and to stream those datasets at high bandwidth to user applications. It is a core component of the Apache Hadoop ecosystem, providing a scalable and fault-tolerant storage solution for Big Data applications.\n",
        "\n",
        "#### HDFS Architecture\n",
        "\n",
        "- **NameNode**: The NameNode is the master server that manages the file system namespace and regulates access to files by clients. It keeps track of the metadata, such as the location, size, and permissions of each file and directory in the file system.\n",
        "\n",
        "- **DataNodes**: DataNodes are the worker nodes in HDFS. They store and manage the actual data blocks that make up the files stored in HDFS. DataNodes periodically report their status to the NameNode and respond to requests from clients to read or write data.\n",
        "\n",
        "- **Secondary Namenode**: Despite its name, the Secondary Namenode does not serve as a backup for the primary NameNode. Instead, it helps in performing periodic checkpoints of the namespace image and merging them with the edit logs to prevent the edit logs from becoming too large. This process helps in reducing the NameNode's startup time.\n",
        "\n",
        "- **Clients**: Clients interact with the NameNode to obtain metadata about the files and directories stored in HDFS. After obtaining this metadata, clients can directly communicate with the DataNodes to read from or write data into HDFS.\n",
        "\n",
        "#### Chunks (Blocks)\n",
        "\n",
        "In HDFS, large files are divided into smaller fixed-size blocks, typically referred to as chunks or blocks. The default size of a block in HDFS is 128 MB, but this can be configured according to the application's requirements. Dividing files into blocks allows HDFS to efficiently distribute and replicate data across multiple DataNodes in the cluster, ensuring high availability and fault tolerance.\n",
        "\n",
        "#### Processing Chunks into Records\n",
        "\n",
        "The process of processing chunks into records depends largely on the type of data and the application using HDFS:\n",
        "\n",
        "- **For structured data**: Applications often define how chunks are interpreted as records based on a predefined schema. For example, in a database context, each chunk might represent a set number of rows or records.\n",
        "  \n",
        "- **For unstructured data**: Applications may process chunks based on custom logic or parsing rules specific to the data format. This could involve parsing files like logs or documents where the boundaries of records are determined by delimiters or patterns.\n",
        "\n",
        "#### Customizing Chunk Processing\n",
        "\n",
        "To customize how chunks are processed into records in HDFS, consider the following approaches:\n",
        "\n",
        "1. **Input Formats**: Hadoop provides various input formats (e.g., TextInputFormat, SequenceFileInputFormat) that determine how chunks are read and processed. Custom input formats can be developed to handle specific data formats or to implement custom processing logic.\n",
        "\n",
        "2. **MapReduce or Spark**: For batch processing frameworks like MapReduce or Apache Spark, developers can define custom mappers to process chunks into records according to specific business logic or data parsing requirements.\n",
        "\n",
        "3. **Streaming Applications**: For real-time processing using frameworks like Apache Flink or Apache Kafka Streams, developers can define how chunks (streaming events) are processed into records (individual events) in real-time, typically using functions or operators that transform the incoming data streams.\n",
        "\n",
        "Customizing chunk processing involves understanding the data format, the requirements of the application, and leveraging the capabilities of the processing framework (like Hadoop MapReduce, Apache Spark, or streaming platforms) to efficiently handle and transform data from chunks into usable records or units of work.\n"
      ],
      "metadata": {
        "id": "sTO2zyC5csQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. Replica in HDFS. Safe mode. NameNode. DataNode.\n",
        "\n",
        "\n",
        "- Safe mode is the read only mode of the NameNode\n",
        "\n",
        "Files and blocks are replicated wile writing:\n",
        "\n",
        "- Same block is stored on several Datanodes\n",
        "- Replicas by default (replication factor) is eq 3 (for example 1 replica would be stored on first Rack local machine second on the same Rack but different machine, and the third replica) Daemon processes:\n",
        "- Namenode - runs on 1-st dedicated machine; responsible for namespace, meta information, where data is stored on cluster.\n",
        "- Datanode - runs on each cluster machine, stores data blocks, sends its state to Namenode\n",
        "- Secondary Namenode - (optional daemon) updates fsimage, logs from datanode. It's not a backup of Namenode\n",
        "\n",
        "Safemode in Apache Hadoop is a maintenance state of NameNode, during which NameNode doesn’t allow any modifications to the file system. In Safemode, HDFS cluster is in read-only and doesn’t replicate or delete Data Blocks.\n",
        "\n",
        "When NameNode starts: It loads the file system namespace from the last saved FsImage into its main memory and the edits log file. Merges edits log file on fsimage and results in new file system namespace. Then it receives block reports containing information about block location from all datanodes. In SafeMode NameNode performs collection of block reports from datanodes. NameNode enters safemode automatically during its start up. NameNode leaves Safemode after the DataNodes have reported that most blocks are available.\n"
      ],
      "metadata": {
        "id": "9Ud1d2UbcvNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. CAP theorem. Communication Protocols. Data consistency.\n",
        "\n",
        "CAP theorem states that any distributed data store can provide only two of the following three guarantees:\n",
        "\n",
        "- Consistency - Every read receives the most recent write or an error.\n",
        "\n",
        "- Availability - Every request receives a (non-error) response, without the guarantee that it contains the most recent write.\n",
        "- Partition tolerance - The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.\n",
        "When a network partition failure happens, it must be decided whether to do one of the following:\n",
        "\n",
        "- cancel the operation and thus decrease the availability but ensure consistency\n",
        "- proceed with the operation and thus provide availability but risk inconsistency.\n",
        "\n",
        "best-effort broadcast protocol guarantees that if the sender does not crash, the message is delivered to all non-faulty processes in a group. A simple way to implement it is to send the message to all processes in a group one-by-one over reliable links but if sender fails mid-way, some processes will never receive the message.\n",
        "\n",
        "reliable broadcast protocol guarantees that the message is eventually delivered to all non-faulty processes in the group, event if the sender crashes before the message has been fully delivered. One way to implement reliable broadcast is to have each processes re-transmit the message to the rest of the group the first time it is delivered. This approach is also known as eager reliable broadcast -ERB. Although it guarantees that all non-faulty processes eventually recieve the message, It’s costly as it requires sending the message N² times for a group of N processes. The number of messages can be reduced by re-transmitting a message on delivery to a random subset of processes.\n",
        "\n",
        "gossip protocol It is a probabilistic protocol, it does not guarantee that a message will be delivered to all processes. That said, it is possible to make that probability negligible by tuning the protocol’s parameters. Gossip protocols are particularly useful when broadcasting to a large number of processes, like thousands or more, where deterministic protocol would not scale.\n",
        "\n",
        "Data consistency is a crucial aspect that ensures the accuracy and reliability of data. So if data is inconsistent, there is nothing right:\n",
        "\n",
        "Data consistency is the accuracy, completeness, and correctness of data stored in a database. The same data across all related systems, applications, and databases is when we say that data is consistent. Inconsistent data can lead to incorrect analysis, decision-making, and outcomes. The key metrics such as accuracy, completeness, timeliness, and relevance are used to analyze or measure data consistency.\n"
      ],
      "metadata": {
        "id": "eUe4qFBM1U2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Data Formats: txt, csv, binary, json, parquet. What's better?\n",
        "\n",
        "TXT - Plain text\n",
        "\n",
        "CSV - Comma Separated Values. Great for tabular data\n",
        "\n",
        "JSON - JavaScript Object Notation. Ideal for the Web applications and APIs needing flexible and human- readable data.\n",
        "\n",
        "Parquet - Columnar storage file format optimized for use with Big Data Processing frameworks. Efficient for Big Data applications requiring efficient storage and fast analytical queries.\n",
        "\n",
        "TXT: Plain text file that contains unformatted text. Great choice for storing and sharing simple, unformatted information, such as notes, lists, or code snippets.\n",
        "\n",
        "CSV: Good option for compatibility, spreadsheet processing and human readable data. The data must be flat. It is not efficient and cannot handle nested data. There may be issues with the separator which can lead to data quality issues. Use this format for exploratory analysis, POCs or small data sets.\n",
        "\n",
        "BINARY: Performance, small size. Not human readable. Advantages in terms of speed of access.\n",
        "\n",
        "JSON: Heavily used in APIs. Nested format. It is widely adopted and human readable but it can be difficult to read if there are lots of nested fields. Great for small data sets, landing data or API integration. If possible convert to more efficient format before processing large amounts of data.\n",
        "\n",
        "Parquet: Columnar storage. It has schema support. It works very well with Hive and Spark as a way to store columnar data in deep storage that is queried using SQL. Because it stores data in columns, query engines will only read files that have the selected columns and not the entire data set as opposed to Avro. Use it as a reporting layer."
      ],
      "metadata": {
        "id": "yGuDx_pZ1dbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Spark"
      ],
      "metadata": {
        "id": "YCC9RNNJ1hb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is Apache Spark? Which components does its architecture consist of?\n",
        "\n",
        "Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
        "\n",
        "Components of Apache Spark Architectures:\n",
        "- Driver Program\n",
        "- SparkContext\n",
        "- Cluster Manager\n",
        "- Executors\n",
        "- Tasks\n",
        "- DAG Scheduler\n",
        "\n",
        "Apache Spark is an open-source framework for large-scale data processing. Unlike some other frameworks, Spark excels at handling a variety of data formats, including structured, semi-structured, and unstructured data. This makes it a versatile tool for tasks like real-time analytics and machine learning.\n",
        "\n",
        "Here's a breakdown of the key components in Spark's architecture:\n",
        "\n",
        "Master-Slave Architecture: Spark follows a master-slave architecture. The master node, called the Driver, coordinates the entire job and communicates with the worker nodes,  called Executors.\n",
        "\n",
        "Cluster Manager: Spark itself is agnostic to the underlying cluster management system. This means it can work with various cluster managers like YARN (from Hadoop) or Mesos to allocate resources across applications.\n",
        "\n",
        "Resilient Distributed Datasets (RDDs): This is a fundamental data structure in Spark. It represents an immutable, distributed collection of elements that can be processed in parallel across the cluster. RDDs are fault-tolerant, meaning Spark can recompute lost data partitions if a worker node fails.\n",
        "\n",
        "Directed Acyclic Graph (DAG): The DAG is the layer responsible for scheduling tasks. The Driver program translates the user's application logic into a DAG, which essentially is a roadmap for executing the entire job efficiently. The DAG breaks down the work into stages, where each stage represents a set of operations that can be run independently.\n",
        "\n",
        "These components work together to enable Spark to efficiently process large datasets in parallel across a cluster of machines."
      ],
      "metadata": {
        "id": "mq2O-E90ahhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is RDD? Which principal components does it consist of? What is a transformation? What is an action?\n",
        "\n",
        "### RDD (Resilient Distributed Dataset)\n",
        "\n",
        "**RDD** (Resilient Distributed Dataset) is a fundamental abstraction in Apache Spark that represents a collection of elements partitioned across the nodes of a cluster, which can be operated on in parallel. RDDs are immutable, fault-tolerant, and can be operated on in a distributed manner across a cluster.\n",
        "\n",
        "#### Principal Components of RDD\n",
        "\n",
        "1. **Partition**: A partition is a logical division of data in an RDD, representing a subset of the dataset. Partitions are distributed across nodes in the cluster and are the unit of parallelism in Spark.\n",
        "\n",
        "2. **Dependencies**: RDDs track lineage information through dependencies. There are two types of dependencies:\n",
        "   - **Narrow dependencies**: Where each partition of the parent RDD is used by at most one partition of the child RDD (e.g., map).\n",
        "   - **Wide dependencies**: Where each partition of the parent RDD may be used by multiple partitions of the child RDD (e.g., reduceByKey).\n",
        "\n",
        "3. **Partitioner**: If an RDD represents a key-value pair and is partitioned, a partitioner controls the distribution of keys across the partitions.\n",
        "\n",
        "#### Transformation\n",
        "\n",
        "**Transformation** in Spark RDDs refers to the operation that creates a new RDD from an existing RDD. Transformations are lazy, meaning they do not compute their results immediately. Instead, they define a lineage of transformations to be applied on the RDDs. Examples of transformations include `map`, `filter`, `flatMap`, `reduceByKey`, `join`, etc.\n",
        "\n",
        "#### Action\n",
        "\n",
        "**Action** in Spark RDDs triggers the execution of the computation defined by the transformations on the RDD. Unlike transformations, actions are eager and return a value (or write data to external storage) after performing the computation on the RDD. Examples of actions include `count`, `collect`, `reduce`, `saveAsTextFile`, `foreach`, etc.\n",
        "\n",
        "Actions are what kick off the computation in Spark. Until an action is called, Spark only builds up a directed acyclic graph (DAG) of transformations, known as the lineage, without executing any computation.\n",
        "\n",
        "RDDs form the foundation of the Spark programming model, allowing developers to perform distributed computations efficiently across large datasets by leveraging transformations and actions to build and execute data processing pipelines.\n"
      ],
      "metadata": {
        "id": "NWiB1zUx7Ltu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is RDD lineage? How is it formed? How can different RDD lineages be combined?\n",
        "\n",
        "RDD lineage is a fundamental concept in Apache Spark that captures the series of transformations leading to an RDD's creation.\n",
        "\n",
        "RDD lineage in Apache Spark refers to the sequence of operations (transformations) that were applied to create an RDD (Resilient Distributed Dataset) from its original source data. It represents the logical execution plan of transformations that will be applied to compute a final result, rather than the actual data itself.\n",
        "\n",
        "Formation of RDD Lineage:\n",
        "1. Initial RDD Creation: RDD lineage begins with the creation of an initial RDD from a data source like a file, HDFS, or by parallelizing a collection in memory.\n",
        "\n",
        "2. Transformation Operations: Subsequent RDDs are derived by applying various transformation operations (like map, filter, reduce, join, etc.) on the existing RDDs. Each transformation results in a new RDD representing the transformed data.\n",
        "\n",
        "3. Lineage Recording: Apache Spark automatically records the lineage of each RDD. If an RDD is lost due to a failure, Spark can use the lineage information to recompute the lost RDD from the original data source.\n",
        "\n",
        "Combining RDD Lineages:\n",
        "\n",
        "Different RDD lineages can be combined using actions like union, intersection, subtract, or join. These actions allow you to merge the data from multiple RDDs into a single RDD or perform set operations.\n",
        "\n",
        "- Union\n",
        "- Intersection\n",
        "- Substract\n",
        "- Join\n",
        "\n",
        "Importance of RDD Lineage:\n",
        "\n",
        "RDD lineage is crucial in Spark because:\n",
        "\n",
        "- Fault Tolerance: It allows Spark to recover lost data partitions by re-executing the lineage from the original data source.\n",
        "- Optimization: Spark can optimize the execution plan (using transformations like map, filter, etc.) based on the lineage to achieve efficient distributed data processing.\n",
        "- Data Immutability: RDDs are immutable, and lineage ensures that each transformation results in a new RDD, preserving data consistency and allowing for easier debugging and fault recovery.\n"
      ],
      "metadata": {
        "id": "nrdooReP7PSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is a partition in RDD? How does a partition in RDD relate to a partition in HDFS?\n",
        "\n",
        "## Partition in RDD\n",
        "\n",
        "1. **Definition**: A partition in Apache Spark RDD (Resilient Distributed Dataset) refers to a logical division of data within the RDD. Partitions allow parallel processing of data across the nodes in a Spark cluster.\n",
        "\n",
        "2. **Characteristics**:\n",
        "   - Each partition is a subset of the RDD's data, processed independently.\n",
        "   - Partitions are immutable once created.\n",
        "   - Spark automatically assigns partitions when RDDs are created, typically based on the number of available cores.\n",
        "\n",
        "3. **Control and Management**:\n",
        "   - Number of partitions can be controlled during RDD creation using operations like `parallelize`, `textFile`, or explicitly with `repartition` or `coalesce` methods.\n",
        "   - Partitioning influences parallelism and task distribution in Spark jobs.\n",
        "\n",
        " Partition in HDFS\n",
        "\n",
        "1. **Definition**: A partition in HDFS (Hadoop Distributed File System) refers to a physical division of data blocks stored across nodes in a Hadoop cluster. It manages data storage and replication.\n",
        "\n",
        "2. **Characteristics**:\n",
        "   - HDFS partitions (data blocks) are managed by the Hadoop NameNode and DataNodes.\n",
        "   - Partitions ensure data distribution, replication, and fault tolerance across the Hadoop cluster.\n",
        "\n",
        "Relationship between RDD Partition and HDFS Partition\n",
        "\n",
        "1. **Logical vs. Physical Division**:\n",
        "   - **RDD Partition**: Logical division of data for parallel processing in Spark.\n",
        "   - **HDFS Partition**: Physical division of data blocks for storage and replication in HDFS.\n",
        "\n",
        "2. **Usage in Spark Jobs**:\n",
        "   - Spark RDDs can be created from data stored in HDFS partitions, automatically partitioning RDDs based on HDFS blocks.\n",
        "   - Aligning RDD partitions with HDFS partitions optimizes data locality and parallelism in Spark.\n",
        "\n",
        "3. **Efficiency Considerations**:\n",
        "   - Efficient processing requires coordinating RDD partitioning with underlying HDFS partitioning.\n",
        "   - Optimal partitioning enhances performance and scalability in big data processing.\n",
        "\n",
        "In summary, while both RDD partitions in Spark and HDFS partitions in Hadoop serve data distribution and processing purposes, they operate at different levels of the data processing stack. RDD partitions handle logical data division and parallel processing within Spark, while HDFS partitions manage physical data storage and replication across the Hadoop cluster.\n"
      ],
      "metadata": {
        "id": "mCWJnSX57Rcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What is broadcast and a broadcasting variable in Apache Spark? What goals does it serve? Give an example of its usage.\n",
        "\n",
        "Broadcast Variable\n",
        "In Apache Spark, a broadcast variable is a read-only variable that is distributed and cached across all worker nodes in a Spark cluster. It allows efficient sharing of large, read-only data structures across multiple tasks in a Spark job. Broadcast variables are particularly useful when the same data needs to be accessed by multiple tasks or stages of computation without being sent over the network multiple times.\n",
        "\n",
        "Goals of Broadcast Variables\n",
        "\n",
        "- Efficiency: Minimizes data transfer over the network by distributing read-only data once to all worker nodes.\n",
        "- Performance: Reduces overhead and improves performance, especially when dealing with large datasets or frequent data access.\n",
        "- Scalability: Facilitates efficient scaling of Spark jobs by optimizing data distribution across the cluster.\n",
        "\n",
        "Benefits of Using Broadcast Variables\n",
        "\n",
        "- Reduces Network Traffic: Instead of transferring large data repeatedly, broadcast variables send data once to each node and reuse it locally.\n",
        "- Improves Performance: Minimizes overhead associated with data distribution, leading to faster job execution.\n",
        "- Optimizes Resource Utilization: Enables efficient utilization of cluster resources by avoiding redundant data transfers.\n",
        "\n",
        "In summary, broadcast variables in Apache Spark are crucial for optimizing performance and resource usage in distributed data processing tasks by facilitating efficient data sharing across multiple tasks or stages. They enhance Spark's capability to handle large datasets and improve overall job execution efficiency."
      ],
      "metadata": {
        "id": "kLfeczZG1wkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation\n",
        "# Initialization: SparkContext is initialized with appropriate\n",
        "# configurations (SparkConf).\n",
        "\n",
        "# Data to Broadcast: data_to_broadcast is a dictionary containing\n",
        "# user IDs as keys and their ages as values. This data is large and read-only.\n",
        "\n",
        "# Broadcasting: sc.broadcast(data_to_broadcast) broadcasts\n",
        "# data_to_broadcast to all worker nodes in the Spark cluster.\n",
        "\n",
        "# Usage: The function process_data accesses the\n",
        "# broadcasted data using broadcast_data.value. It retrieves the age of a user based on the user_id parameter passed to it.\n",
        "\n",
        "# Parallel Processing:\n",
        "# sc.parallelize(user_ids).map(process_data).collect() demonstrates parallel\n",
        "# processing across multiple user_ids. Each task can efficiently access the\n",
        "# broadcasted data_to_broadcast.\n",
        "\n",
        "# Output: The results are collected and printed, showing the processed information for each user ID.\n",
        "\n",
        "# Shutdown: sc.stop() shuts down the Spark context after the job completes.\n",
        "\n",
        "# from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Initialize Spark context\n",
        "conf = SparkConf().setAppName(\"Broadcast Example\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Example of a large dataset (to be broadcasted)\n",
        "data_to_broadcast = {\"user1\": 25, \"user2\": 30, \"user3\": 28}\n",
        "\n",
        "# Broadcast the data\n",
        "broadcast_data = sc.broadcast(data_to_broadcast)\n",
        "\n",
        "# Function to use the broadcast variable\n",
        "def process_data(user_id):\n",
        "    age = broadcast_data.value.get(user_id, \"Age not found\")\n",
        "    return f\"User {user_id} has age {age}\"\n",
        "\n",
        "# Example usage\n",
        "user_ids = [\"user1\", \"user2\", \"user4\"]\n",
        "\n",
        "results = sc.parallelize(user_ids).map(process_data).collect()\n",
        "\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# Stop Spark context\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "id": "VyLZVmaAtzIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. What is caching and persistence in Apache Spark? What is the difference between caching and persistence in Apache Spark? Which modes of persistence exist? What is spillover?\n",
        "\n",
        "Caching\n",
        "\n",
        "Caching in Apache Spark is a mechanism that allows storing RDDs (Resilient Distributed Datasets) or DataFrames in memory to speed up the execution of future actions or transformations on the same dataset. When an RDD or DataFrame is cached, Spark keeps it in memory, avoiding the need to recompute it from scratch each time it is referenced.\n",
        "\n",
        "Persistence\n",
        "\n",
        "Persistence is a more flexible version of caching. While caching typically stores data in memory only, persistence allows storing data in different storage levels (e.g., memory, disk, or both). The persist() method in Spark allows specifying the storage level explicitly.\n",
        "\n",
        "Difference between Caching and Persistence\n",
        "\n",
        "- Caching: A shorthand for persist() using the default storage level (MEMORY_ONLY). It is convenient when you want to store data in memory for quick access.\n",
        "\n",
        "- Persistence: Offers more control over how data is stored, allowing different storage levels such as MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY, etc. This flexibility is useful when working with large datasets that might not fit entirely in memory.\n",
        "\n",
        "Modes of Persistence (Storage Levels)\n",
        "\n",
        "- MEMORY_ONLY: Stores RDD or DataFrame in memory. If it does not fit in memory, recomputation is required.\n",
        "- MEMORY_AND_DISK: Stores RDD or DataFrame in memory. If it does not fit, the remaining data is stored on disk.\n",
        "- MEMORY_ONLY_SER: Stores RDD or DataFrame as serialized objects in memory. Reduces memory usage but increases CPU overhead for serialization.\n",
        "- MEMORY_AND_DISK_SER: Stores RDD or DataFrame as serialized objects in memory and spills over to disk if necessary.\n",
        "- DISK_ONLY: Stores RDD or DataFrame on disk only.\n",
        "- OFF_HEAP: Stores RDD or DataFrame in off-heap memory (requires extra configuration).\n",
        "\n",
        "Spillover\n",
        "\n",
        "Spillover occurs when the data being cached or persisted does not fit entirely in memory and has to be written to disk. This is common with storage levels like MEMORY_AND_DISK or MEMORY_AND_DISK_SER. Spillover helps prevent out-of-memory errors by providing a fallback storage option on disk, though it comes with performance overhead due to disk I/O operations."
      ],
      "metadata": {
        "id": "mR5kACZ-tZey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. What is shuffling? Explain how it works.\n",
        "\n",
        "What is Shuffling?\n",
        "\n",
        "Shuffling in Apache Spark is a mechanism that involves redistributing data across the cluster. It occurs when a transformation requires data to be re-partitioned or aggregated across different nodes, which often leads to data movement between partitions. Shuffling is an expensive operation in terms of performance and resource usage because it involves disk I/O, network I/O, and serialization.\n",
        "\n",
        "How Shuffling Works\n",
        "\n",
        "Shuffling typically happens during operations that require grouping or aggregating data across partitions. Here are some common operations that trigger shuffling:\n",
        "\n",
        "- groupByKey()\n",
        "- reduceByKey()\n",
        "- sortByKey()\n",
        "- join()\n",
        "- distinct()\n",
        "\n",
        "The shuffling process can be broken down into several steps:\n",
        "\n",
        "Mapper Phase:\n",
        "\n",
        "Each node performs the map-side operations, where the data is transformed, and intermediate key-value pairs are created.\n",
        "Intermediate data is stored in memory and sometimes spilled to disk if it does not fit in memory.\n",
        "\n",
        "Partitioning:\n",
        "\n",
        "The intermediate data is partitioned based on the target RDD’s partitioning strategy (e.g., hash partitioning, range partitioning).\n",
        "Each key-value pair is assigned to a partition based on the partitioning function.\n",
        "\n",
        "Shuffle Write:\n",
        "\n",
        "The partitioned data is written to local disk, creating shuffle files.\n",
        "Each node writes out its partitioned data intended for other nodes.\n",
        "\n",
        "Shuffle Read:\n",
        "\n",
        "Each node reads the shuffle files created by all other nodes that have data for its partitions.\n",
        "This step involves network I/O as data is transferred across the cluster.\n",
        "\n",
        "Reducer Phase:\n",
        "\n",
        "The nodes process the shuffled data, performing the reduce-side operations (e.g., aggregation, sorting).\n",
        "The final output is generated and stored in the respective partitions.\n"
      ],
      "metadata": {
        "id": "J8p99_OKtaxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSQL"
      ],
      "metadata": {
        "id": "LBq67JkE12LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is SparkSQL? What is SparkSession? What is DataFrame? What is Column? Name differences between RDD and DataFrame. What is Tungsten?\n",
        "\n",
        "- SparkSQL is the module of Apache Spark for structured data processing.\n",
        "\n",
        "With SparkSQL we can work with SQL, and DataFrame API.\n",
        "\n",
        "Spark Session - the entry point to programming Spark with the Dataset and DataFrame API."
      ],
      "metadata": {
        "id": "rNq6JYNA403H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "  .appName('ExampleApp')\n",
        "  .getOrCreate()"
      ],
      "metadata": {
        "id": "1ZHSYu2F7Lrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DataFrame in SparkSQL is a distributed collection of data organized into named columns = data frame in R/Python.\n",
        "\n",
        "Data Frame can be created by Hive, external databases, or existing RDDs."
      ],
      "metadata": {
        "id": "w4zYbRUX7W6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "XcmEj7eG8AhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Difference between RDD and dataframe in Spark\n",
        "\n",
        "1. DataFrame is faster, higher- level, schema- based, and optimized operations.\n",
        "\n",
        "2. RDD is low- level, unoptimized, and schema-less"
      ],
      "metadata": {
        "id": "zkb6T2UY8BGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tungsten - A project to enhance Spark's performance through optimizations in memory management, binary processing and code generation\n",
        "\n",
        " Within the Apache Spark framework for big data processing, \"Tungsten\" refers to an internal projectcodename. It focuses on optimizing Spark's execution engine to improve memory and CPU usage. This translates to faster processing of data, especially for Spark applications."
      ],
      "metadata": {
        "id": "aCph5TQS8gMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mL5I2oEM8yGy",
        "outputId": "10847550-1a30-487c-c26c-86f575acaec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0b0f5173225facf9f88b49aad33000bc9ad54f034bf34603ce6becfe2226e005\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TungstenExample\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a simple DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29), (\"David\", 42), (\"Eve\", 36)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# Perform some transformations\n",
        "df_filtered = df.filter(col(\"Age\") > 30)\n",
        "df_avg_age = df_filtered.agg(avg(\"Age\").alias(\"Average Age\"))\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "print(\"Filtered DataFrame with Average Age:\")\n",
        "df_avg_age.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MbQkI_s780h4",
        "outputId": "a4dc1bd6-b2c7-4425-870a-001c13f59d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|      Bob| 45|\n",
            "|Catherine| 29|\n",
            "|    David| 42|\n",
            "|      Eve| 36|\n",
            "+---------+---+\n",
            "\n",
            "Filtered DataFrame with Average Age:\n",
            "+-----------+\n",
            "|Average Age|\n",
            "+-----------+\n",
            "|      39.25|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. UDF in Spark SQL. How to write an UDF (give a simple example)? What pitfalls should you be aware about when writing a custom udf?\n",
        "\n",
        "\n",
        "- UDF: User- defined Function\n",
        "\n",
        "UDF is a custom function that we can create and register to extend the capabilities of SparkSQL.\n",
        "\n",
        "Allow to apply custom logic to the data within DataFrames and SQL queries\n",
        "\n",
        "### User-Defined Functions (UDFs) in Spark SQL\n",
        "\n",
        "**UDFs** (User-Defined Functions) in Spark SQL allow you to extend the functionality of Spark by applying custom transformations on your data. UDFs are particularly useful when you need to apply complex logic that is not covered by built-in Spark SQL functions.\n",
        "\n",
        "#### How to Write a UDF (Simple Example)\n",
        "\n",
        "To write a UDF in Spark SQL, you typically define a function in your programming language (e.g., Scala, Python, Java) and then register it with Spark SQL.\n",
        "\n",
        "Pitfalls When Writing Custom UDFs\n",
        "When writing custom UDFs in Spark SQL, be aware of the following pitfalls:\n",
        "\n",
        "- Performance: Inefficient UDFs can lead to poor performance, especially when they involve complex computations or operations that require shuffling of data across the cluster.\n",
        "\n",
        "- Serialization: Ensure that all objects used within the UDF are Serializable, especially if your UDF depends on external libraries or non-serializable objects.\n",
        "\n",
        "- Type Safety: Define the input and output types of your UDF correctly. Mismatched types or inconsistent handling of null values can lead to unexpected behavior or errors.\n",
        "\n",
        "- Optimization: Spark performs optimization on built-in functions but may not optimize custom UDFs in the same way. Consider using built-in functions where possible for better performance.\n",
        "\n",
        "- Compatibility: UDFs defined in one language (e.g., Scala) may not be directly usable in another (e.g., Python). Ensure compatibility and test across different environments if necessary.\n",
        "\n",
        "- Version Compatibility: UDF behavior and APIs may vary between different versions of Spark. Verify compatibility when upgrading Spark versions.\n",
        "\n",
        "By understanding these pitfalls and best practices, you can effectively leverage UDFs in Spark SQL to extend its capabilities and perform custom data transformations efficiently.\n"
      ],
      "metadata": {
        "id": "HVmJ2PZT6fQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating sparkSession and dataframe\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('UDFExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)"
      ],
      "metadata": {
        "id": "9m0ERFCb-Jpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def age_group(age):\n",
        "    if age < 35:\n",
        "        return \"Young\"\n",
        "    else:\n",
        "        return \"Adult\"\n",
        "\n",
        "# Register the function as a UDF\n",
        "age_group_udf = udf(age_group, StringType())\n",
        "\n",
        "\n",
        "df_with_age_group = df.withColumn(\"AgeGroup\", age_group_udf(df[\"Age\"]))\n",
        "df_with_age_group.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1cmSg6E_FDK",
        "outputId": "f54226e4-eaa1-4845-a6fb-c5e3ef0a9ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+--------+\n",
            "|     Name|Age|AgeGroup|\n",
            "+---------+---+--------+\n",
            "|    Alice| 34|   Young|\n",
            "|      Bob| 45|   Adult|\n",
            "|Catherine| 29|   Young|\n",
            "+---------+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pitfalls to be aware of when writing a custom UDF:\n",
        "\n",
        "- Performance overhead: it is slow. And the data should be serialized and deserialized between JVM and Python, and it can be expensive\n",
        "\n",
        "- Optimization: Lack of Optimization, which means they can be less efficient compared to built-in functions.\n",
        "\n",
        "- Debugging: Debugging is challenging due to their custom nature and execution in a distributed environment.\n",
        "\n",
        "- Reusability issues: We cannot reuse it and the custom function is just for the session that we registered."
      ],
      "metadata": {
        "id": "DCDCFZ3m_6HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is a schema of DataFrame? How to get a schema of DataFrame? What is groupby and how does it work?\n",
        "\n",
        "- A schema is the structure of the data, including the columna names, data types, and nullable properties.\n",
        "\n",
        "\"printSchema\" to get the schema of DataFrame.\n",
        "\n",
        "groupby Operation\n",
        "\n",
        "The groupby operation in Apache Spark allows you to perform grouping of data based on one or more columns. It is similar to the SQL GROUP BY clause and is used for aggregating data within each group.\n",
        "\n",
        "How groupby Works\n",
        "\n",
        "When you apply groupby to a DataFrame, you specify one or more columns by which you want to group the data. Spark then organizes the rows of the DataFrame into groups based on unique combinations of values in the specified columns. After grouping, you typically apply aggregation functions (such as sum, avg, count, etc.) to compute summary statistics for each group."
      ],
      "metadata": {
        "id": "rZNyTWZ86hrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('schemaExample') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", \"HR\", 34), (\"Bob\", \"IT\", 45), (\"Catherine\", \"HR\", 29), (\"David\", \"IT\", 42), (\"Eve\", \"Finance\", 36)]\n",
        "columns = [\"Name\", \"Department\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "df_grouped = df.groupby(\"Department\").agg(avg('Age').alias('Average Age'))\n",
        "df_grouped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZH-oy_rBn0Z",
        "outputId": "40fcfaa0-fa5c-4d26-cf87-474abb2898ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|Department|Average Age|\n",
            "+----------+-----------+\n",
            "|        HR|       31.5|\n",
            "|        IT|       43.5|\n",
            "|   Finance|       36.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz_5Bo75CUtM",
        "outputId": "a5ddda1a-28e3-4757-eab6-08fcd8f4fbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Average Age: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Supported join operations in Spark SQL. Broadcast joins. Basic principles of functioning. Which types of broadcast-based joins exist and what are the differences in their functioning? Conditions of when one can apply these types of joins.\n",
        "\n",
        "- Inner Join. Returns rows that have matching values in both tables\n",
        "\n",
        "- Left Outer Join (Left Join) - returns all rows from the left table, and the matched rows from the right table.\n",
        "\n",
        "- Right Join - from the right table, and matched left values\n",
        "\n",
        "- Full Join -  Returns all rows when there is a match in one of the tables. Rows without matches in the other table will have NULL values.\n",
        "\n",
        "- Cross Join (Cartesian Product): Returns the Cartesian product of both tables."
      ],
      "metadata": {
        "id": "6fKvyKA06khS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcast Joins are a type of join where one of the smaller DataFrame is broadcast to all worker nodes. This can significantly improve the performance of joins involving a large and a small DataFrame by avoiding the need for data shuffling.\n",
        "\n",
        "Basic Principles of Functioning:\n",
        "1. Broadcasting\n",
        "2. Joining\n",
        "3. Efficiency\n",
        "\n",
        "Types of Joins:\n",
        "1. Broadcast Hash Join (BHJ)\n",
        "2. Broadcast Loop join\n",
        "\n",
        "\n",
        "Conditions for Applying Broadcast joins:\n",
        "1. Size of DataFrame - dataframe should be small enough to fit into the memory of each worker node\n",
        "2. Spark Configuration - spark.sql.autoBroadcastJoinThreshold\n",
        "3. Join Conditions: equi- joins = joins with equal conditions"
      ],
      "metadata": {
        "id": "CFAQXCHADiVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcast Join\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "spark = SparkSession.builder.appName('BroadcastJoinExample').getOrCreate()\n",
        "\n",
        "df1 = spark.createDataFrame([(1, 'A'), (2, 'B'), (3, 'C')], [\"id\", \"value1\"])\n",
        "df2 = spark.createDataFrame([(1, 'X'), (2, 'Y')], [\"id\", \"value2\"])\n",
        "\n",
        "df1.join(broadcast(df2), df1.id == df2.id).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymLkae7CZ7T-",
        "outputId": "b357e996-35e0-4532-a08f-2d770aaa0d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+------+\n",
            "| id|value1| id|value2|\n",
            "+---+------+---+------+\n",
            "|  1|     A|  1|     X|\n",
            "|  2|     B|  2|     Y|\n",
            "+---+------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JoinExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create DataFrames\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 1000),\n",
        "    (2, \"Bob\", 1500),\n",
        "    (3, \"Charlie\", 1200),\n",
        "    (4, \"David\", 1800)\n",
        "], [\"emp_id\", \"emp_name\", \"salary\"])\n",
        "\n",
        "departments = spark.createDataFrame([\n",
        "    (1, \"HR\"),\n",
        "    (2, \"Engineering\"),\n",
        "    (3, \"Finance\")\n",
        "], [\"dept_id\", \"dept_name\"])\n",
        "\n",
        "# Show DataFrames\n",
        "print(\"Employees DataFrame:\")\n",
        "employees.show()\n",
        "\n",
        "print(\"Departments DataFrame:\")\n",
        "departments.show()\n",
        "\n",
        "# Perform Join Operations\n",
        "\n",
        "# 1. Inner Join\n",
        "inner_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"inner\")\n",
        "print(\"Inner Join Result:\")\n",
        "inner_join_df.show()\n",
        "\n",
        "# 2. Left Outer Join (Left Join)\n",
        "left_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"left_outer\")\n",
        "print(\"Left Outer Join Result:\")\n",
        "left_join_df.show()\n",
        "\n",
        "# 3. Right Outer Join (Right Join)\n",
        "right_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"right_outer\")\n",
        "print(\"Right Outer Join Result:\")\n",
        "right_join_df.show()\n",
        "\n",
        "# 4. Full Outer Join (Full Join)\n",
        "full_join_df = employees.join(departments, employees.emp_id == departments.dept_id, \"outer\")\n",
        "print(\"Full Outer Join Result:\")\n",
        "full_join_df.show()\n",
        "\n",
        "# 5. Cross Join (Cartesian Product)\n",
        "cross_join_df = employees.crossJoin(departments)\n",
        "print(\"Cross Join Result:\")\n",
        "cross_join_df.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WF6W_5Cajlh",
        "outputId": "576ff296-3e56-433d-b00a-bea6e0a38bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees DataFrame:\n",
            "+------+--------+------+\n",
            "|emp_id|emp_name|salary|\n",
            "+------+--------+------+\n",
            "|     1|   Alice|  1000|\n",
            "|     2|     Bob|  1500|\n",
            "|     3| Charlie|  1200|\n",
            "|     4|   David|  1800|\n",
            "+------+--------+------+\n",
            "\n",
            "Departments DataFrame:\n",
            "+-------+-----------+\n",
            "|dept_id|  dept_name|\n",
            "+-------+-----------+\n",
            "|      1|         HR|\n",
            "|      2|Engineering|\n",
            "|      3|    Finance|\n",
            "+-------+-----------+\n",
            "\n",
            "Inner Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Left Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|   NULL|       NULL|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Right Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Full Outer Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|   NULL|       NULL|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n",
            "Cross Join Result:\n",
            "+------+--------+------+-------+-----------+\n",
            "|emp_id|emp_name|salary|dept_id|  dept_name|\n",
            "+------+--------+------+-------+-----------+\n",
            "|     1|   Alice|  1000|      1|         HR|\n",
            "|     2|     Bob|  1500|      1|         HR|\n",
            "|     1|   Alice|  1000|      2|Engineering|\n",
            "|     1|   Alice|  1000|      3|    Finance|\n",
            "|     2|     Bob|  1500|      2|Engineering|\n",
            "|     2|     Bob|  1500|      3|    Finance|\n",
            "|     3| Charlie|  1200|      1|         HR|\n",
            "|     4|   David|  1800|      1|         HR|\n",
            "|     3| Charlie|  1200|      2|Engineering|\n",
            "|     3| Charlie|  1200|      3|    Finance|\n",
            "|     4|   David|  1800|      2|Engineering|\n",
            "|     4|   David|  1800|      3|    Finance|\n",
            "+------+--------+------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Sort Merge Join. What is it for? Basic principles of functioning. Conditions of when one can apply these types of joins. Advantages and disadvantages in comparison with other joins.\n",
        "\n",
        "Sort Merge Join\n",
        "\n",
        "Sort Merge Join is a type of join operation used in database systems and distributed data processing frameworks like Apache Spark. It combines two datasets by sorting them based on join keys and then merging them in a sorted order.\n",
        "\n",
        "Purpose\n",
        "\n",
        "Sort Merge Join is used to efficiently combine two datasets (or tables) based on a common key, especially when both datasets are already sorted by the join key or can be sorted relatively quickly.\n",
        "\n",
        "Basic Principles of Functioning\n",
        "\n",
        "1. Sorting: Both input datasets are sorted based on the join key. This sorting can be achieved using efficient sorting algorithms.\n",
        "\n",
        "2. Merging: Once sorted, the datasets are merged together by comparing the sorted values of the join keys:\n",
        "\n",
        "- Start with pointers (or iterators) at the beginning of both sorted lists.\n",
        "- Compare the current values at the pointers.\n",
        "- Based on the comparison, move the pointers forward:\n",
        "-If the values are equal, create a join record.\n",
        "- If one value is less than the other, move the pointer of that dataset forward.\n",
        "- Continue until all matching records are processed.\n",
        "\n",
        "3. Join Execution: During the merge phase, matching records from both datasets are combined into the result set.\n",
        "\n",
        "Conditions for Applying Sort Merge Join\n",
        "Sort Merge Join is effective under the following conditions:\n",
        "\n",
        "- Both datasets are sorted: This is essential for the merge phase to efficiently combine the datasets.\n",
        "- Comparable size: Sort Merge Join performs well when both datasets are roughly the same size.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- Efficiency: Can be more efficient than other join algorithms when both datasets are pre-sorted.\n",
        "- Predictable Performance: It has a predictable performance characteristic, typically O(n log n) for sorting followed by O(n) for merging.\n",
        "- Scalability: Suitable for large datasets if sorting can be efficiently distributed.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "S- orting Overhead: If datasets are not initially sorted, sorting them can introduce overhead in terms of time and resources.\n",
        "- Memory Requirements: Sorting large datasets requires sufficient memory, which can be a limitation in distributed environments.\n",
        "- Not Always Applicable: Sorting may not always be feasible or efficient, especially if datasets cannot be sorted due to size or format.\n",
        "\n",
        "Comparison with Other Joins\n",
        "\n",
        "- Hash Join: Sort Merge Join requires sorting both datasets, whereas Hash Join can proceed without sorting if a hash table can be built on one dataset.\n",
        "- Nested Loop Join: Sort Merge Join is typically more efficient than Nested Loop Join for large datasets due to better time complexity.\n",
        "- Performance: Sort Merge Join can outperform other joins when both datasets are pre-sorted, but Hash Join might be preferable when sorting is not feasible.\n",
        "\n",
        "Sort Merge Join is a powerful algorithm for efficiently combining sorted datasets based on a common key, offering predictable performance and efficiency in data processing systems."
      ],
      "metadata": {
        "id": "T6nPLQBC6nAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Bucketing: what it is, its advantages and basic principles of functioning.\n",
        "\n",
        "\n",
        "Bucketing is a technique used in Apache Spark to optimize data storage and query performance by organizing data into a specified number of buckets based on the hash value of a column's value.\n",
        "\n",
        "What It Is\n",
        "\n",
        "Bucketing involves dividing data into a fixed number of buckets (or partitions) based on the hash value of a specific column. Each bucket contains data rows that share the same hash value modulo the number of buckets. This allows for efficient data retrieval and processing when querying data based on the bucketed column.\n",
        "\n",
        "Advantages\n",
        "\n",
        "- Improved Query Performance: Bucketing can significantly improve query performance, especially for operations that involve grouping, joining, or aggregating data based on the bucketed column. It reduces the amount of data that needs to be scanned and processed.\n",
        "\n",
        "- Data Organization: It provides a structured way to organize data, making it easier to manage and analyze large datasets.\n",
        "\n",
        "- Optimized Joins: When joining two tables bucketed on the same join key, the query optimizer can perform a map-side join, which avoids shuffling data across the network and improves performance.\n",
        "\n",
        "Basic Principles of Functioning:\n",
        "\n",
        "1. Bucketing Columns: You select one or more columns on which to bucket your data. Typically, this is done on columns that are frequently used in queries for grouping or joining.\n",
        "\n",
        "2. Hash Function: A hash function is applied to the bucketing column's value to determine which bucket a row belongs to. This hash value is then used to determine the bucket number (bucket_id = hash(column_value) % num_buckets).\n",
        "\n",
        "3. Data Distribution: Rows with the same hash value modulo the number of buckets are placed in the same bucket.\n",
        "\n",
        "4. Query Optimization: When querying bucketed tables, the query planner can optimize by reading only the relevant buckets, reducing the amount of data scanned and improving query performance."
      ],
      "metadata": {
        "id": "nu56XFDb6o68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BucketingExample\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Catherine\", 29), (\"David\", 42), (\"Eve\", 36)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Bucketing Configuration\n",
        "num_buckets = 4\n",
        "bucket_column = \"Age\"\n",
        "df_bucketed = df.repartitionByRange(num_buckets, bucket_column)\n",
        "\n",
        "# Show the bucketed DataFrame\n",
        "df_bucketed.show()\n",
        "\n",
        "# Stop SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twkXpXIMckLL",
        "outputId": "a28d312a-ccee-4eb3-f0c5-b3734c50ef57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+\n",
            "|     Name|Age|\n",
            "+---------+---+\n",
            "|    Alice| 34|\n",
            "|Catherine| 29|\n",
            "|      Eve| 36|\n",
            "|    David| 42|\n",
            "|      Bob| 45|\n",
            "+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClickHouse"
      ],
      "metadata": {
        "id": "g01UoBuc2WbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.What is ClickHouse? Which components does its architecture consist of?\n",
        "\n",
        "ClickHouse is an open-source columnar database management system (DBMS) designed for real-time analytics. It was developed by Yandex and is widely used for handling large-scale data processing workloads efficiently. ClickHouse is known for its high performance, capable of processing terabytes to petabytes of data with low latency.\n",
        "\n",
        "- It is used for the online analytical processing (OLAP) and real- time analytics on large volumes of data\n",
        "\n",
        "Key features:\n",
        "- High performance\n",
        "- Columnar storage. Data is stored in columns, not rows and it makes faster\n",
        "\n",
        "Architecture:\n",
        "1. Server:\n",
        "\n",
        "ClickHouse Server: The core component responsible for managing databases, processing queries, and coordinating interactions between other components. It handles incoming SQL queries, manages connections, and orchestrates data processing tasks.\n",
        "2. Storage:\n",
        "\n",
        "- Columnar Storage: ClickHouse uses a columnar storage format where data in each column is stored contiguously. This format is optimized for analytical queries that typically access a subset of columns rather than entire rows.\n",
        "Data Skipping Index: ClickHouse employs a data skipping index (MergeTree) that allows skipping unnecessary data during query execution, improving query performance by minimizing disk reads.\n",
        "3. Query Processing:\n",
        "\n",
        "Vectorized Query Execution: ClickHouse performs query execution in a vectorized manner, processing data in batches (vectors) rather than row by row. This approach enhances performance by leveraging modern CPU architectures and reducing overhead.\n",
        "4. Distributed Processing:\n",
        "\n",
        "- Distributed Tables: ClickHouse supports distributed tables, allowing data to be partitioned and distributed across multiple nodes in a cluster. Distributed queries are coordinated by the ClickHouse Server, which ensures efficient data retrieval and aggregation across nodes.\n",
        "- Replication and Fault Tolerance: ClickHouse provides mechanisms for data replication and fault tolerance to ensure high availability and durability of data. It supports replication across multiple nodes and handles node failures gracefully.\n",
        "5. Integration and Interfaces:\n",
        "\n",
        "- SQL Interface: ClickHouse supports SQL as its primary query language, making it compatible with existing tools and frameworks that work with SQL databases.\n",
        "- APIs and Connectors: ClickHouse offers APIs and connectors (HTTP, native client libraries) for integrating with various programming languages and applications.\n",
        "6. Management and Monitoring:\n",
        "\n",
        "- System Tables: ClickHouse includes system tables that store metadata and statistics about databases, tables, and query performance. These system tables are crucial for monitoring and optimizing database operations.\n",
        "- ClickHouse HTTP Interface: Provides a web-based interface for querying system tables, monitoring cluster health, and managing database configurations."
      ],
      "metadata": {
        "id": "BiGxscrN55iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.What is Materialized View and what is its purpose? How does Materialized Views work? Does it make sense to create a materialized view over a distributed table? If the answer is positive, why?\n",
        "\n",
        "- Materialized Views in ClickHouse is a database object that contains the results of a query.\n",
        "\n",
        "Purpose of Materialized Views:\n",
        "- Performance Optimization\n",
        "- Efficient Aggregation\n",
        "\n",
        "How it works?\n",
        "- Creation: \"Create Materialized View\"\n",
        "- Data Ingestion: as new data is inserted into the source table, the materialized view is automatically updated.\n",
        "\n",
        "- Storage: The results of the query are stored on disk"
      ],
      "metadata": {
        "id": "mMhLtUqFB_zB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Materialized view over Distributed Tables:\n",
        "\n",
        "Yes, it makes sense, because:\n",
        "- Precomputed results: Aggregations and complex computations can be precomputed, reducing the load on distributed query execution\n",
        "\n",
        "- Performance: avoiding repeated calculations\n",
        "\n",
        "- Consistency: Maintaining data consistency by updated sync with the dist. source table"
      ],
      "metadata": {
        "id": "F6SRkPOxGE1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Describe MergeTree engine work principles. How does ClickHouse store data in the filesystem? How does parts merging works from in terms of files?\n",
        "\n",
        "- MergeTree Engine: A high-performance storage engine in ClickHouse, supporting efficient data ingestion, storage, and querying with columnar storage and primary key indexing.\n",
        "\n",
        "## MergeTree Engine Work Principles\n",
        "\n",
        "The MergeTree engine in ClickHouse is designed for handling large datasets and high data ingestion rates. Here's a breakdown of its core principles:\n",
        "\n",
        "* **Time Partitioning:** Data is divided into partitions based on time. This keeps data with similar timestamps together for efficient queries and data operations on specific timeframes.\n",
        "* **Data Sorting:** Each partition is sorted by the table's primary key. This allows ClickHouse to quickly locate specific data points using efficient search algorithms.\n",
        "* **Data Merging:** When new data arrives, it's inserted into the appropriate partition based on the primary key. If the new data overlaps with existing data in a partition, a merge operation is triggered. This process combines the overlapping parts into a larger, more optimized partition.\n",
        "* **Data Compression:** MergeTree can optionally compress data during the merge operation to reduce disk space usage. The compression algorithm is chosen based on the data's characteristics.\n",
        "* **Uniqueness Support:** MergeTree can enforce data uniqueness within partitions, ensuring no duplicate rows exist based on the primary key.\n",
        "\n",
        "## ClickHouse Data Storage in Filesystem\n",
        "\n",
        "ClickHouse uses the local filesystem by default to store data. Alternatively, it can leverage remote storage options like Amazon S3. Here's how data gets organized:\n",
        "\n",
        "* **Tables:** Each table using the MergeTree engine has a dedicated directory on the filesystem.\n",
        "* **Partitions:** Inside the table directory, data is further divided into subdirectories representing time partitions.\n",
        "* **Parts:** Each partition consists of multiple files called \"parts.\" These parts store the actual data in a columnar format, where each column has its own separate file. Alternatively, ClickHouse can use a \"compact format\" where all columns are stored in a single file per part.  \n",
        "* **Granules:** The smallest unit of data that ClickHouse reads from disk is a \"granule.\" It typically consists of 8192 rows (configurable) for a specific column. This minimizes disk I/O operations by reading data in batches.\n",
        "\n",
        "## Merge Operation and File Management\n",
        "\n",
        "The merge operation combines overlapping parts within a partition to optimize storage and performance. Here's what happens during a merge:\n",
        "\n",
        "1. ClickHouse identifies the overlapping parts based on their timeframes and primary key ranges.\n",
        "2. Data from the overlapping parts is read and sorted together based on the primary key.\n",
        "3. Duplicate rows are removed if uniqueness is enforced.\n",
        "4. The merged data is compressed (if configured) and written to a new part file.\n",
        "5. The old, overlapping parts are marked for deletion. These are eventually cleaned up by a background process.\n",
        "\n",
        "By continuously merging parts, ClickHouse keeps the data organized and reduces storage footprint while maintaining efficient query performance.\n",
        "\n",
        "\n",
        "Work Principles of Merge Tree Engine:\n",
        "1. Data Ingestion- Uploading data\n",
        "2. Primary Key and Indexing -\n",
        "3. Storage of Data in Filesystem\n",
        "\n",
        "How parts Mergin Works:\n",
        "1. Background Merging: Clickhouse continuously merges smaller parts into larger parts in the background, a process known as compaction\n",
        "\n",
        "\n",
        "2. Merge Algorithm\n",
        "3. Files Involved in Merging"
      ],
      "metadata": {
        "id": "EgXEVVI_CE8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is sharding, why is it useful and how is it implemented in ClickHouse? How can one distribute data into shards using ClickHouse?\n",
        "\n",
        "Sharding is a database architecture pattern that involves partitioning data across multiple servers or nodes to distribute the load and improve performance. Each partition is called a shard. In the context of ClickHouse, sharding helps manage large datasets efficiently by distributing them across multiple nodes in a cluster.\n",
        "\n",
        "Why Sharding is Useful\n",
        "1. Scalability: By distributing data across multiple nodes, ClickHouse can handle larger datasets and higher query loads than a single node could manage.\n",
        "2. Performance: Queries can be processed in parallel on different shards, significantly speeding up query execution times.\n",
        "3. Fault Tolerance: If one node fails, the system can continue to operate using the remaining nodes, improving the reliability of the database.\n",
        "4. Resource Management: Sharding allows for better utilization of resources by spreading the computational and storage load across multiple nodes\n",
        "\n",
        "Implementation of Sharding in ClickHouse\n",
        "\n",
        "Sharding in ClickHouse is implemented using a Distributed table, which acts as a logical representation of several underlying tables (one per shard). The Distributed engine distributes the data across multiple shards based on a sharding key.\n",
        "\n",
        "Setting Up Sharding in ClickHouse\n",
        "\n",
        "1. Create Local Tables on Each Node: These tables will store the actual data and will be created on each node in the cluster.\n",
        "2. Create a Distributed Table: This table will serve as the unified entry point for queries and will distribute the data across the local tables based on the sharding key.\n",
        "\n",
        "Distributing Data into Shards\n",
        "The sharding key determines how data is distributed across the shards. It should be chosen carefully to ensure an even distribution of data and avoid hotspots. Common strategies for sharding keys include:\n",
        "\n",
        "- Hash-Based Sharding: Uses a hash function to distribute data. For example, using cityHash64(product_id) as the sharding key ensures that rows with the same product_id are stored on the same shard.\n",
        "- Range-Based Sharding: Divides data into ranges based on a key. For example, you could shard data by date to ensure that each shard contains data for a specific date range.\n",
        "\n",
        "- Random Sharding: Uses a random function like rand() to distribute data. This is simple but may not always result in an even distribution."
      ],
      "metadata": {
        "id": "uHL1YrQXCHTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. How does AggregatingMergeTree work? Are there any differences between aggregation functions in AggregatingMergeTree and simple queries?\n",
        "\n",
        "AggregatingMergeTree is a specialized storage engine in ClickHouse designed for efficient aggregation of data during the merging process. It is particularly useful for scenarios where pre-aggregated data can save significant computational resources and improve query performance.\n",
        "\n",
        "How AggregatingMergeTree Works\n",
        "\n",
        "- Primary Key and Order: Similar to other MergeTree engines, AggregatingMergeTree requires a primary key and defines an order in which data is sorted.\n",
        "\n",
        "- Aggregation During Ingestion: When data is inserted into an AggregatingMergeTree table, it is initially stored in a raw, unaggregated form.\n",
        "\n",
        "- Aggregation During Merging: The real power of AggregatingMergeTree comes into play during the merging process. As data parts are merged (a background process in ClickHouse), the engine applies aggregation functions to the data. This reduces the amount of stored data by aggregating rows with the same primary key values.\n",
        "\n",
        "- Materialized View: To take full advantage of AggregatingMergeTree, you often use materialized views to insert data in a pre-aggregated form directly into the table.\n",
        "\n",
        "Differences Between Aggregation Functions in AggregatingMergeTree and Simple Queries\n",
        "\n",
        "Aggregation in AggregatingMergeTree:\n",
        "- Pre-Aggregation: Aggregation is performed during the merge process, which means that data is stored in an already aggregated form. This reduces the computational load during query execution.\n",
        "\n",
        "- Aggregation Functions: Special aggregation functions are used, which are designed to work with AggregatingMergeTree. These functions are applied during the merging process to combine rows with the same primary key.\n",
        "\n",
        "- Performance: Since data is pre-aggregated, query performance is significantly improved because fewer rows need to be scanned and aggregated at query time.\n",
        "\n",
        "- Use Case: Ideal for scenarios where frequent, similar aggregation queries are run on large datasets, such as time-series data or event logs.\n",
        "\n",
        "Aggregation in Simple Queries:\n",
        "- On-the-Fly Aggregation: Aggregation is performed at query time. Each query computes the aggregation from the raw data stored in the table.\n",
        "\n",
        "- Standard Aggregation Functions: Uses standard SQL aggregation functions like SUM, AVG, COUNT, MIN, MAX, etc.\n",
        "\n",
        "- Performance: Can be slower for large datasets, as each query must scan and aggregate data from scratch.\n",
        "\n",
        "- Use Case: Suitable for ad-hoc queries or when the data is not frequently queried in an aggregated form."
      ],
      "metadata": {
        "id": "2hpThfAOCI8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 6. Why is it recommended inserting data into ClickHouse by large batches? What can one do if data arrives in short portions frequently? Name and explain existing mechanisms in ClickHouse to deal with the problem.\n",
        "\n",
        "Inserting data into ClickHouse in large batches is recommended due to the following reasons:\n",
        "\n",
        "- Efficiency: Large batches are more efficient because they reduce the overhead associated with each insert operation. This includes network latency, transaction management, and file system overhead.\n",
        "\n",
        "- Compression: ClickHouse achieves better compression rates with larger data sets. Compressing large blocks of data together results in higher compression ratios, reducing storage requirements and improving read performance.\n",
        "\n",
        "- Merge Operations: ClickHouse uses a process called \"merging\" to organize and optimize data on disk. Inserting data in large batches minimizes the frequency of these merge operations, thus reducing the system's overall load.\n",
        "\n",
        "- Resource Utilization: Larger batches make better use of system resources, such as CPU and memory, by processing more data per operation and reducing the number of idle cycles.\n",
        "\n",
        "Handling Frequent Small Portions of Data:\n",
        "\n",
        "1. Buffer Table Engine:  The Buffer engine allows temporary storage of incoming data in memory, which is periodically flushed to the destination table in larger batches.\n",
        "\n",
        "2. Kafka Table Engine: ClickHouse can consume data from Kafka topics, which can be batched and inserted into ClickHouse tables. This is useful for streaming data and ensures that small data portions are accumulated and inserted efficiently.\n",
        "\n",
        "3. Distributed Table: Using a Distributed table can help spread small inserts across multiple shards, reducing the load on a single node and allowing for parallel processing of insertions.\n",
        "\n",
        "4. Batching Logic in Application: Implementing batching logic within the application layer before sending data to ClickHouse can also help. This involves accumulating small inserts in memory or temporary storage and sending them as larger batches to ClickHouse.\n",
        "\n",
        "5. Using Materialized Views: Materialized views can pre-aggregate data and insert it into target tables in larger chunks, optimizing storage and query performance."
      ],
      "metadata": {
        "id": "-p2F_uXSCKph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Describe sparse index work principles. How is it implemented in ClickHouse? How can we set up indexes in ClickHouse?\n",
        "\n",
        "Sparse Index is an indexing technique used by ClickHouse to quickly locate data within large datasets without needing to scan the entire table. This type of index doesn't store every value but rather stores pointers to blocks of data, enabling efficient data retrieval.\n",
        "\n",
        "Principles of Sparse Index\n",
        "1. Data Blocks: ClickHouse stores data in columns, and these columns are further divided into smaller parts called blocks.\n",
        "2. Primary Key: When a table is created with a primary key, ClickHouse creates a sparse index on this key. The primary key determines how the data is sorted and indexed.\n",
        "3. Index Granularity: The index does not store every value of the primary key but instead stores every nth value (determined by the index granularity). This means that the index provides pointers to blocks where the actual data resides.\n",
        "4. Efficient Data Retrieval: When querying data, ClickHouse uses the sparse index to quickly skip over blocks that do not contain relevant data, thereby speeding up the query process.\n",
        "\n",
        "Implementation in ClickHouse:\n",
        "\n",
        "1. Primary Key Index: When creating a table, defining a primary key automatically creates a sparse index on that key.\n",
        "\n",
        "2. Custom Indices: ClickHouse allows the creation of additional indices on other columns using various index types, like minmax, set, bloom_filter, etc.\n",
        "\n",
        "Setting Up Indexes in ClickHouse\n",
        "\n",
        "Indexes in ClickHouse can be set up during table creation or added to existing tables using the ALTER TABLE command.\n",
        "\n"
      ],
      "metadata": {
        "id": "uPwMAiSLCMJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kubernetes"
      ],
      "metadata": {
        "id": "1D9jUmwq2niK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is Kubernetes? Its architecture, main components.\n",
        "\n",
        "Kubernetes: Orchestrating Containerized Applications\n",
        "Kubernetes, often abbreviated as K8s, is an open-source system for automating the deployment, scaling, and management of containerized applications. It groups containerized applications into logical units called Pods and manages them across a cluster of machines.\n",
        "\n",
        "Architecture\n",
        "Kubernetes follows a client-server architecture with two main planes:\n",
        "\n",
        "Control Plane:  The brain of the operation, responsible for making decisions and issuing commands. It consists of several components working together:\n",
        "\n",
        "API Server: The central point of communication, accepting user requests and commands through a RESTful API.\n",
        "Scheduler: Assigns Pods to worker nodes based on defined criteria like resource availability.\n",
        "\n",
        "Controller Manager: Manages the lifecycle of Pods, Deployments, and other Kubernetes resources.\n",
        "etcd: A highly available key-value store that stores the shared state of the cluster configuration.\n",
        "Data Plane: The worker bees that execute the actual workloads. It consists of:\n",
        "\n",
        "Worker Nodes: Machines (physical or virtual) that run containerized applications. Each node has a:\n",
        "Kubelet: Agent responsible for receiving commands from the control plane and managing Pods on its node.\n",
        "Container Runtime: Software responsible for running containers (e.g., Docker, containerd).\n",
        "\n",
        "Pods: The smallest deployable unit in Kubernetes. A Pod typically contains one or more containers that share storage and network resources.\n",
        "Services: Abstracts deployment details and provides a way to access applications running on Pods across the cluster.\n",
        "\n",
        "1. Master Components:\n",
        "- API server\n",
        "- etcd. Key- value store used for all cluster data storage\n",
        "- Controller Manager\n",
        "- Scheduler\n",
        "\n",
        "2. Worker Node Components:\n",
        "- Kubelet\n",
        "- Kube- Proxy\n",
        "- Container Runtime\n",
        "\n",
        "Main Components:\n",
        "\n",
        "Here's a breakdown of the key components in Kubernetes:\n",
        "\n",
        "- API Server: The central point of communication for Kubernetes. It accepts requests and commands from users and applications through a RESTful API.\n",
        "\n",
        "- Scheduler: Responsible for assigning Pods to worker nodes in the cluster based on defined criteria like resource availability and Pod requirements.\n",
        "- Controller Manager: A collection of controllers that manage the lifecycle of various Kubernetes objects like deployments, ReplicaSets, and Services. It ensures the desired state of the cluster is maintained.\n",
        "- etcd: A highly available key-value store that stores the shared state of the cluster configuration. It serves as the single source of truth for all cluster data.\n",
        "- Worker Nodes: Machines (physical or virtual) that run containerized applications. Each worker node runs a Kubelet and a container runtime.\n",
        "- Kubelet: Agent running on each worker node. It receives commands from the API server and manages the lifecycle of Pods assigned to the node.\n",
        "- Container Runtime: Software responsible for running containers on the worker node. Examples include Docker, containerd, and CRI-O.\n",
        "- Pods: The smallest deployable unit in Kubernetes. A Pod typically contains one or more containers that share storage (volume) and network resources.\n",
        "- Services: Logical abstractions that provide a way to access applications running on Pods across the cluster. They act as a load balancer, directing traffic to the appropriate Pods based on a defined selection criteria (e.g., round-robin)."
      ],
      "metadata": {
        "id": "i2VQROHz7dBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. What is Pod? Difference between pod and container.\n",
        "\n",
        "Pod - smallest and simplest unit of deployment. It represents the group of one or more containers that share storage (volumes), network, and lifecycle settings.\n",
        "\n",
        "Difference between container and pod:\n",
        "Pods:\n",
        "- Scope: A Pod can contain multiple containers that are tightly coupled and share resources. Containers, on the other hand, are standalone units that encapsulate a single process or service.\n",
        "- Networking: Containers within a Pod share the same network namespace and can communicate via localhost. Containers in different Pods require network communication through Kubernetes networking.\n",
        "- Management: Pods are managed by Kubernetes and can be created, scaled, and deleted as a unit. Containers are managed within Pods and are not directly orchestrated by Kubernetes outside of the Pod context.\n",
        "\n",
        "\n",
        "A container is a standard unit of software that packages up code and all its dependencies, so the application runs quickly and reliably from one computing environment to another.\n"
      ],
      "metadata": {
        "id": "SfHLTe6sUrn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is Service? How does it work? Service types: nodeport, clusterip, headless service.\n",
        "\n",
        "In Kubernetes, a Service is an abstraction that defines a logical set of Pods and a policy by which to access them. It provides a stable endpoint (IP address and port) for connecting to a group of Pods, regardless of their individual IP addresses or the nodes they run on. Services enable communication between various components in a Kubernetes cluster and external clients.\n",
        "\n",
        "Service Types:\n",
        "- NodePort - Exposes the Service on each Node's IP address at a static port. It creates a high-port (30000-32767) on each Node that redirects to the Service.\n",
        "- ClusterIp - This IP address is accessible only within the Kubernetes cluster. The default internal port.\n",
        "- LoadBalancer. External IP address thhat routes traffic to the Service\n",
        "- Headless service - The ClusterIP is set to None and no virtual IP is allocated. Useful for stateful applications where each Pod neds a unique identity\n",
        "\n",
        "How a Service Works:\n",
        "\n",
        "1. Service Definition: You define a Service object in your Kubernetes manifest that specifies:\n",
        "\n",
        "- Selector: A label selector to identify the Pods that the Service should expose.\n",
        "- Ports: The ports on which the Service listens for incoming traffic. These ports are typically mapped to container ports within the selected Pods.\n",
        "- Service Type: The type of Service that determines how external clients access the Pods (e.g., NodePort, ClusterIP, Headless).\n",
        "\n",
        "2. Service Discovery: Kubernetes implements service discovery using an internal component called kube-proxy. Kube-proxy maintains a mapping of Services and their corresponding Pods on each worker node.\n",
        "\n",
        "3. Client Traffic Routing: When a client sends a request to the Service's IP address and port (defined by the Service type), kube-proxy on the appropriate worker node intercepts the request.\n",
        "\n",
        "4. Load Balancing (Optional): Depending on the Service type, kube-proxy may perform load balancing by distributing traffic across multiple Pods matching the selector."
      ],
      "metadata": {
        "id": "j9wMghgWUvZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 4. Container resources: how is it implemented? CPU, RAM, Storage. QoS classes.\n",
        "\n",
        "1. CPU and RAM: Managed using requests and limits to ensure predictable scheduling and prevent resource starvation.\n",
        "2. Storage: Managed through Persistent Volumes and Persistent Volume Claims, allowing persistent storage for applications.\n",
        "3.  QOS (Quality of Service) Classes - Assigns Pods to Guaranteed, Burstable, or BestEffort classes based on their resource requests and usage patterns, ensuring fair resource allocation in the cluster.\n",
        "- Guaranteed: Pods have both CPU and memory requests set and these requests equal their limits\n",
        "\n",
        "- Burstable: The usage of CPU and RAM exceeds the limits and requests\n",
        "\n",
        "- BestEffort: Pods doesnt have the requests set and they receive lowest priority after other pods.\n"
      ],
      "metadata": {
        "id": "Byq2RsfMUw_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 5. PV & PVC: what is it and how does it work? PV types. Volume provisioner.\n",
        "\n",
        "Persistent Volume (PV) - A storage resource in a Kubernetes Cluster. They are resources in the cluster just like nodes and cluster resources.\n",
        "\n",
        "Persistent Volume Claim (PVC) - A user's request for storage resources that binds to a PV.\n",
        "\n",
        "PV types:\n",
        "- Manual: Created by adminstrators\n",
        "- Dynamic: Automatically and dynamically provisioned based on StorageClasses.\n",
        "\n",
        "Volume Provisioner: A component that automates the creation of PVs based on user-defined StorageClasses.\n",
        "\n",
        "Common Provisioners\n",
        "- kubernetes.io/aws-ebs: AWS Elastic Block Store\n",
        "- kubernetes.io/gce-pd: Google Compute Engine Persistent Disk\n",
        "- kubernetes.io/azure-disk: Azure Disk\n",
        "- kubernetes.io/nfs: Network File System\n",
        "- kubernetes.io/host-path: Local storage on the host"
      ],
      "metadata": {
        "id": "gTrS26OmUy1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. StatefulSet. Difference from Deployment/Pod.\n",
        "\n",
        "## StatefulSets vs Deployments vs Pods in Kubernetes\n",
        "\n",
        "In Kubernetes, Pods, Deployments, and StatefulSets are all fundamental objects used to manage containerized applications. However, they cater to different use cases based on whether the application is stateful or stateless.\n",
        "\n",
        "### Pods\n",
        "\n",
        "* **What it is:** The smallest deployable unit in Kubernetes. A Pod typically contains one or more containers that share storage (volume) and network resources.\n",
        "* **Use case:** Pods are ideal for simple, stateless applications that don't require persistent storage or a specific network identity.\n",
        "* **Example:** A web server that serves static content.\n",
        "\n",
        "### Deployments\n",
        "\n",
        "* **What it is:** A higher-level abstraction that manages Pods and their updates. Deployments ensure desired state is reached by creating new Pods and phasing out old ones in a controlled manner.\n",
        "* **Stateful vs Stateless:**  Deployments are primarily designed for stateless applications. They treat Pods as interchangeable units.\n",
        "* **Key Features:**\n",
        "    * **Rolling Updates:** Deployments can perform rolling updates with minimal downtime by creating new Pods with the updated container image while gracefully terminating old Pods.\n",
        "    * **Scaling:** Deployments can be easily scaled up or down by adjusting the desired number of replicas.\n",
        "* **Services:** Deployments typically work in conjunction with Services to provide a stable network identity for the application even as Pods are recreated.\n",
        "* **Example:** A horizontally scaled application with multiple instances of a web application served through a LoadBalancer Service.\n",
        "\n",
        "### StatefulSets\n",
        "\n",
        "* **What it is:** A controller that manages the deployment and scaling of stateful applications. Unlike Deployments, StatefulSets treat Pods as unique entities with specific network identities and persistent storage.\n",
        "* **Stateful vs Stateless:** StatefulSets are designed specifically for stateful applications that require persistent storage and a stable network identity to maintain state across restarts or scaling events.\n",
        "* **Key Features:**\n",
        "    * **Ordered, Stateful Deployment:** StatefulSets ensure Pods are created and scaled in a specific order, preserving the desired state of the application.\n",
        "    * **Persistent Storage:** StatefulSets can leverage Persistent Volumes (PVs) to provide persistent storage for Pods, ensuring data is not lost during restarts or pod rescheduling.\n",
        "    * **Stable Network Identity:** StatefulSets maintain a stable network identity (hostname and DNS entry) for each Pod, even if it's rescheduled to a different node.\n",
        "* **Example:** A database cluster where each Pod represents a database instance with its own persistent storage and unique hostname.\n",
        "\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "| Feature | Pod | Deployment (Stateless) | StatefulSet (Stateful) |\n",
        "|---|---|---|---|\n",
        "| Type | Smallest deployable unit | Manages Pods and updates | Manages stateful Pods |\n",
        "| Stateful | No | No | Yes |\n",
        "| Network Identity | Ephemeral | Can change with pod recreation | Stable, unique per Pod |\n",
        "| Storage | Ephemeral | Ephemeral or volume attached | Persistent (PVs) |\n",
        "\n",
        "\n",
        "In essence, Pods are the building blocks, Deployments manage scaling and updates for stateless applications, and StatefulSets manage scaling and updates for stateful applications with persistent storage needs and stable network identities.  \n"
      ],
      "metadata": {
        "id": "KLgcyOfBU0P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 7. Label selector. What is it? How is it used?\n",
        "\n",
        "Label selector are a core consept in Kubernetes used to organize and select subsets of objects on their labels.\n",
        "\n",
        "Labels are key- value pairs attached to Kubernetes objects, such as Pods, Nodes, Services.\n",
        "\n",
        "Types:\n",
        "1. Equality- Based Selectors - selects exact match key- value\n",
        "2. Set- Based Selectors - match labels based on set operations\n",
        "\n",
        "Usage: Widely used in various Kubernetes resources like Services, ReplicaSets, Jobs, Network Policies, and more, to dynamically group and manage objects efficiently.\n",
        "\n",
        "By using label selectors, We can dynamically manage and control Kubernetes resources, making it easier to scale, organize, and maintain your applications.\n",
        "\n",
        "Benefits of Label Selectors:\n",
        "\n",
        "- Flexible Grouping: Labels and selectors enable dynamic grouping of resources based on shared attributes, regardless of their physical location or resource type.\n",
        "- Declarative Management: Selectors define the desired state rather than specific Pod names, making your configurations more portable and scalable.\n",
        "- Improved Maintainability: Using labels and selectors simplifies managing complex deployments with many resources.\n"
      ],
      "metadata": {
        "id": "mL1g--4WU1pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7iHEtxiWb9q"
      },
      "outputs": [],
      "source": []
    }
  ]
}